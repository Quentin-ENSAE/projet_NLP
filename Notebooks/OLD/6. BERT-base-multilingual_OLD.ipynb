{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Barre de progression pour Jupyter\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import s3fs\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/projet_NLP\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/onyxia/work/projet_NLP\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('data/df_train.pkl')\n",
    "data_test = pd.read_pickle('data/df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # doit renvoyer True\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))  # nom de ton GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of French dataset train to training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.filter(items=['case_text', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952    [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
       "3615      [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False]\n",
       "1883      [False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False]\n",
       "4767         [False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, True, False, False, False]\n",
       "576      [False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Cas_clinique', 'MeSH_Code'], dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_fr = pd.read_csv(\"GEMINI/df_train_fr.csv\")\n",
    "df_train_fr.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_fr = df_train_fr.filter(items=['Cas_clinique', 'MeSH_Code'])\n",
    "df_train_fr = df_train_fr.rename(columns={\"Cas_clinique\": \"case_text\", \"MeSH_Code\": \"target\"})\n",
    "df_train_fr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [04, 08, 12]\n",
      "1    [01, 08, 20]\n",
      "2        [05, 07]\n",
      "3    [06, 17, 23]\n",
      "4        [08, 16]\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def extract_mesh_codes(lst):\n",
    "    \"\"\"\n",
    "    À partir d'une liste de strings ['C04 – neoplasms', ...],\n",
    "    renvoie ['04', '08', ...] (sans le 'C').\n",
    "    \"\"\"\n",
    "    codes = []\n",
    "    for item in lst:\n",
    "        # on découpe sur l’en-dash « – » ou le tiret-moins « - »\n",
    "        raw = item.split('–')[0].split('-')[0].strip()\n",
    "        # si ça commence par 'C', on retire la lettre\n",
    "        if raw.upper().startswith('C') and len(raw) > 1:\n",
    "            num = raw[1:]\n",
    "        else:\n",
    "            num = raw\n",
    "        codes.append(num)\n",
    "    return codes\n",
    "\n",
    "def process_target_string(s):\n",
    "    \"\"\"\n",
    "    Transforme la chaîne s, qui représente une liste Python,\n",
    "    en réelle liste, puis en extrait les codes numériques sans 'C'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lst = ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "    return extract_mesh_codes(lst)\n",
    "\n",
    "# Application à votre DataFrame\n",
    "df_train_fr['target'] = df_train_fr['target'].apply(process_target_string)\n",
    "\n",
    "# Vérification\n",
    "print(df_train_fr['target'].head())\n",
    "# Ex. : [['04', '08', '12'], ['01', '08', '20'], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['C04 – neoplasms', 'C08 – respiratory tract diseases', 'C12 – urologic and male genital diseases']\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_fr['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Script import multilabel_preprocessing as mp\n",
    "df_train_fr['target'] = df_train_fr['target'].apply(extract_mesh_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_fr['target'] = df_train_fr['target'].apply(mp.vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monsieur Dupont, âgé de 72 ans, consulte son médecin traitant pour des troubles urinaires évoluant depuis plusieurs mois. Il se plaint principalement d'une augmentation de la fréquence des mictions, y compris la nuit (nycturie), d'une difficulté à initier la miction et d'un jet urinaire faible. Il note également une sensation de vidange incomplète de la vessie après avoir uriné.\\n\\nMonsieur Dupont a des antécédents d'hypertension artérielle traitée par inhibiteur de l'enzyme de conversion (IEC) et de tabagisme chronique (40 paquets-années), bien qu'il ait arrêté de fumer il y a 10 ans. Il n'a pas d'antécédents familiaux de cancer de la prostate.\\n\\nL'examen clinique révèle un abdomen souple et non douloureux. La palpation sus-pubienne ne révèle pas de globe vésical. Le toucher rectal met en évidence une prostate augmentée de volume, de consistance ferme et régulière, sans nodule suspect. Un dosage du PSA (antigène prostatique spécifique) est prescrit et revient à 7,5 ng/mL (valeur normale &lt; 4 ng/mL).\\n\\nCompte tenu de ces résultats et des symptômes du patient, une consultation urologique est demandée. L'urologue réalise un examen complémentaire comprenant un débitmétrie urinaire qui confirme une diminution significative du débit maximal (Qmax). Une échographie de la vessie avec mesure du résidu post-mictionnel révèle un résidu important (&gt; 150 mL).\\n\\nFace à la suspicion d'hypertrophie bénigne de la prostate (HBP) associée à un PSA élevé, l'urologue propose la réalisation de biopsies prostatiques échoguidées. Les biopsies révèlent la présence d'un adénocarcinome prostatique de grade Gleason 7 (3+4) dans plusieurs carottes. Un bilan d'extension est réalisé, comprenant une scintigraphie osseuse et une IRM pelvienne, qui ne mettent pas en évidence de métastases à distance.\\n\\nMonsieur Dupont est informé du diagnostic de cancer de la prostate localisé de risque intermédiaire. Une discussion multidisciplinaire est organisée pour déterminer la meilleure option thérapeutique. Compte tenu de son âge, de ses comorbidités et de ses préférences, une prostatectomie radicale robot-assistée est proposée. L'intervention se déroule sans complications notables. L'examen histopathologique de la pièce opératoire confirme le diagnostic d'adénocarcinome prostatique avec marges chirurgicales saines. Des ganglions lymphatiques pelviens sont prélevés et ne présentent pas de métastases. \\n\\nEn post-opératoire, Monsieur Dupont présente une incontinence urinaire d'effort transitoire, qui s'améliore progressivement avec des séances de rééducation périnéale. Un suivi régulier du PSA est mis en place pour surveiller l'évolution de la maladie. \\n\\nTrois ans après l'intervention, Monsieur Dupont consulte pour une dyspnée progressive. Une radiographie thoracique révèle une image suspecte au niveau du poumon droit. Une TDM thoracique confirme la présence d'un nodule pulmonaire. Une biopsie de ce nodule révèle une métastase du cancer de la prostate. Une hormonothérapie est alors initiée pour contrôler la progression de la maladie.</td>\n",
       "      <td>[False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mme. Rivière, âgée de 55 ans et présentant des antécédents d'asthme mal contrôlé et d'allergie aux acariens, s'est présentée à la clinique avec une toux persistante, une dyspnée croissante et une fièvre légère durant depuis environ une semaine. Elle a également rapporté une sensation de serrement thoracique et une production accrue de mucus jaunâtre. Ses symptômes se sont progressivement aggravés malgré l'utilisation de son inhalateur de secours (salbutamol). \\n\\nÀ l'examen clinique, on notait une respiration sifflante diffuse et des râles crépitants aux deux bases pulmonaires. Sa saturation en oxygène était à 90% sous air ambiant. Une radiographie thoracique a révélé des infiltrats bilatéraux, plus prononcés dans les lobes inférieurs, suggérant une pneumonie. \\n\\nUn test de dépistage rapide de la grippe s'est avéré négatif. Cependant, une analyse des expectorations a révélé la présence de Streptococcus pneumoniae. Des prélèvements sanguins ont également montré une élévation des marqueurs inflammatoires (CRP et leucocytes). \\n\\nCompte tenu de la suspicion de pneumonie bactérienne compliquant son asthme, un traitement antibiotique par amoxicilline-acide clavulanique a été instauré par voie orale. Des bronchodilatateurs (salbutamol et ipratropium) ont été administrés par nébulisation pour soulager la bronchoconstriction. En raison de la saturation en oxygène persistante en dessous de 92%, une oxygénothérapie a été initiée par lunettes nasales. \\n\\nMalgré le traitement initial, l'état de Mme. Rivière s'est détérioré dans les 48 heures suivantes. Sa dyspnée s'est intensifiée et elle a développé une fièvre plus élevée. Une nouvelle radiographie thoracique a montré une progression des infiltrats pulmonaires et l'apparition d'un épanchement pleural. Des hémocultures ont confirmé la présence de Streptococcus pneumoniae résistant à la pénicilline. \\n\\nLe traitement antibiotique a été modifié pour une céphalosporine de troisième génération (ceftriaxone) par voie intraveineuse. Une ponction pleurale a été réalisée, drainant un liquide purulent qui s'est révélé positif pour Streptococcus pneumoniae. Un drain thoracique a été inséré pour assurer un drainage continu de l'épanchement pleural. \\n\\nDe plus, une évaluation immunologique a été réalisée, révélant un déficit en IgG2, une sous-classe d'immunoglobulines importante pour la réponse aux infections bactériennes encapsulées comme le pneumocoque. Un traitement d'immunoglobulines intraveineuses (IgIV) a été envisagé pour renforcer son système immunitaire. \\n\\nAprès une hospitalisation prolongée et des ajustements thérapeutiques, l'état de Mme. Rivière s'est progressivement amélioré. Elle a pu être sevrée de l'oxygène et le drain thoracique a été retiré. Elle a été renvoyée à domicile avec un traitement antibiotique oral prolongé et un suivi régulier en pneumologie et immunologie.</td>\n",
       "      <td>[True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     case_text  \\\n",
       "0  Monsieur Dupont, âgé de 72 ans, consulte son médecin traitant pour des troubles urinaires évoluant depuis plusieurs mois. Il se plaint principalement d'une augmentation de la fréquence des mictions, y compris la nuit (nycturie), d'une difficulté à initier la miction et d'un jet urinaire faible. Il note également une sensation de vidange incomplète de la vessie après avoir uriné.\\n\\nMonsieur Dupont a des antécédents d'hypertension artérielle traitée par inhibiteur de l'enzyme de conversion (IEC) et de tabagisme chronique (40 paquets-années), bien qu'il ait arrêté de fumer il y a 10 ans. Il n'a pas d'antécédents familiaux de cancer de la prostate.\\n\\nL'examen clinique révèle un abdomen souple et non douloureux. La palpation sus-pubienne ne révèle pas de globe vésical. Le toucher rectal met en évidence une prostate augmentée de volume, de consistance ferme et régulière, sans nodule suspect. Un dosage du PSA (antigène prostatique spécifique) est prescrit et revient à 7,5 ng/mL (valeur normale < 4 ng/mL).\\n\\nCompte tenu de ces résultats et des symptômes du patient, une consultation urologique est demandée. L'urologue réalise un examen complémentaire comprenant un débitmétrie urinaire qui confirme une diminution significative du débit maximal (Qmax). Une échographie de la vessie avec mesure du résidu post-mictionnel révèle un résidu important (> 150 mL).\\n\\nFace à la suspicion d'hypertrophie bénigne de la prostate (HBP) associée à un PSA élevé, l'urologue propose la réalisation de biopsies prostatiques échoguidées. Les biopsies révèlent la présence d'un adénocarcinome prostatique de grade Gleason 7 (3+4) dans plusieurs carottes. Un bilan d'extension est réalisé, comprenant une scintigraphie osseuse et une IRM pelvienne, qui ne mettent pas en évidence de métastases à distance.\\n\\nMonsieur Dupont est informé du diagnostic de cancer de la prostate localisé de risque intermédiaire. Une discussion multidisciplinaire est organisée pour déterminer la meilleure option thérapeutique. Compte tenu de son âge, de ses comorbidités et de ses préférences, une prostatectomie radicale robot-assistée est proposée. L'intervention se déroule sans complications notables. L'examen histopathologique de la pièce opératoire confirme le diagnostic d'adénocarcinome prostatique avec marges chirurgicales saines. Des ganglions lymphatiques pelviens sont prélevés et ne présentent pas de métastases. \\n\\nEn post-opératoire, Monsieur Dupont présente une incontinence urinaire d'effort transitoire, qui s'améliore progressivement avec des séances de rééducation périnéale. Un suivi régulier du PSA est mis en place pour surveiller l'évolution de la maladie. \\n\\nTrois ans après l'intervention, Monsieur Dupont consulte pour une dyspnée progressive. Une radiographie thoracique révèle une image suspecte au niveau du poumon droit. Une TDM thoracique confirme la présence d'un nodule pulmonaire. Une biopsie de ce nodule révèle une métastase du cancer de la prostate. Une hormonothérapie est alors initiée pour contrôler la progression de la maladie.   \n",
       "1                                                                                                                                                                                         Mme. Rivière, âgée de 55 ans et présentant des antécédents d'asthme mal contrôlé et d'allergie aux acariens, s'est présentée à la clinique avec une toux persistante, une dyspnée croissante et une fièvre légère durant depuis environ une semaine. Elle a également rapporté une sensation de serrement thoracique et une production accrue de mucus jaunâtre. Ses symptômes se sont progressivement aggravés malgré l'utilisation de son inhalateur de secours (salbutamol). \\n\\nÀ l'examen clinique, on notait une respiration sifflante diffuse et des râles crépitants aux deux bases pulmonaires. Sa saturation en oxygène était à 90% sous air ambiant. Une radiographie thoracique a révélé des infiltrats bilatéraux, plus prononcés dans les lobes inférieurs, suggérant une pneumonie. \\n\\nUn test de dépistage rapide de la grippe s'est avéré négatif. Cependant, une analyse des expectorations a révélé la présence de Streptococcus pneumoniae. Des prélèvements sanguins ont également montré une élévation des marqueurs inflammatoires (CRP et leucocytes). \\n\\nCompte tenu de la suspicion de pneumonie bactérienne compliquant son asthme, un traitement antibiotique par amoxicilline-acide clavulanique a été instauré par voie orale. Des bronchodilatateurs (salbutamol et ipratropium) ont été administrés par nébulisation pour soulager la bronchoconstriction. En raison de la saturation en oxygène persistante en dessous de 92%, une oxygénothérapie a été initiée par lunettes nasales. \\n\\nMalgré le traitement initial, l'état de Mme. Rivière s'est détérioré dans les 48 heures suivantes. Sa dyspnée s'est intensifiée et elle a développé une fièvre plus élevée. Une nouvelle radiographie thoracique a montré une progression des infiltrats pulmonaires et l'apparition d'un épanchement pleural. Des hémocultures ont confirmé la présence de Streptococcus pneumoniae résistant à la pénicilline. \\n\\nLe traitement antibiotique a été modifié pour une céphalosporine de troisième génération (ceftriaxone) par voie intraveineuse. Une ponction pleurale a été réalisée, drainant un liquide purulent qui s'est révélé positif pour Streptococcus pneumoniae. Un drain thoracique a été inséré pour assurer un drainage continu de l'épanchement pleural. \\n\\nDe plus, une évaluation immunologique a été réalisée, révélant un déficit en IgG2, une sous-classe d'immunoglobulines importante pour la réponse aux infections bactériennes encapsulées comme le pneumocoque. Un traitement d'immunoglobulines intraveineuses (IgIV) a été envisagé pour renforcer son système immunitaire. \\n\\nAprès une hospitalisation prolongée et des ajustements thérapeutiques, l'état de Mme. Rivière s'est progressivement amélioré. Elle a pu être sevrée de l'oxygène et le drain thoracique a été retiré. Elle a été renvoyée à domicile avec un traitement antibiotique oral prolongé et un suivi régulier en pneumologie et immunologie.   \n",
       "\n",
       "                                                                                                                                                                                target  \n",
       "0  [False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]  \n",
       "1  [True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_fr.to_pickle(\"data/df_train_fr.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BERT-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496fc5d4507c4e71bb436de8102dd64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d071f3d2694a989feffc094c219671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13141fe72804560bc71b12590722f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f194f4bd51ce40fea9e84f4e5ac6d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744669cceac742a9b600f43c6f0bfb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Modèle bilingue FR/EN (multilingual BERT)\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Nombre de classes en sortie\n",
    "n_labels = 26  # à ajuster selon ton jeu de données\n",
    "\n",
    "# Modèle pour classification multi-label\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=n_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     9646.000000\n",
      "mean       908.879121\n",
      "std        641.935476\n",
      "min         34.000000\n",
      "25%        501.000000\n",
      "50%        770.000000\n",
      "75%       1128.000000\n",
      "max      14186.000000\n",
      "Name: n_tokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculer le nombre de tokens pour chaque case_text\n",
    "stat=pd.DataFrame()\n",
    "stat['n_tokens'] = data_train['case_text'].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "\n",
    "# 2. Afficher les stats descriptives\n",
    "print(stat['n_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10718/10718 [00:04<00:00, 2313.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "df = data_train.rename(columns={\"target\": \"labels\"})\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"case_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset_train_val = \u001b[43mdataset\u001b[49m.train_test_split(test_size=\u001b[32m0.1\u001b[39m/\u001b[32m0.9\u001b[39m) \u001b[38;5;66;03m# 0.1 /0.9 pour avoir meme taille de validation set et de test set\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtaille dataset entrainement :\u001b[39m\u001b[33m\"\u001b[39m, dataset_train_val[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m0\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtaille dataset validation :\u001b[39m\u001b[33m\"\u001b[39m, dataset_train_val[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "dataset_train_val = dataset.train_test_split(test_size=0.1/0.9) # 0.1 /0.9 pour avoir meme taille de validation set et de test set\n",
    "\n",
    "print(\"taille dataset entrainement :\", dataset_train_val[\"train\"].shape[0])\n",
    "print(\"taille dataset validation :\", dataset_train_val[\"test\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  11/3350 00:06 < 41:35, 1.34 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     44\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m'\u001b[39m: best_threshold,\n\u001b[32m     45\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmicro_precision\u001b[39m\u001b[33m'\u001b[39m: precision_score(labels, preds, average=\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m, zero_division=\u001b[32m0\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mexact_match_accuracy\u001b[39m\u001b[33m'\u001b[39m: exact_match_accuracy(labels, preds)\n\u001b[32m     50\u001b[39m     }\n\u001b[32m     52\u001b[39m trainer = Trainer(\n\u001b[32m     53\u001b[39m     model=model,\n\u001b[32m     54\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2565\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2560\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2569\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results_BERT_ml\",\n",
    "    eval_strategy=\"epoch\",              # évaluation à chaque époque\n",
    "    save_strategy=\"epoch\",                    # sauvegarde à chaque époque (utile pour reprise)\n",
    "    save_total_limit=2,                       # limite le nombre de checkpoints\n",
    "    learning_rate=3e-5,                       # légèrement augmenté pour convergence plus rapide\n",
    "    per_device_train_batch_size=4,            # réduit à 4 pour éviter OOM (GPU 16 Go)\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,                       # plus d'époques si dataset pas trop gros\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                                # active le mixed precision training (optimisé pour Ampere)\n",
    "    gradient_accumulation_steps=4,            # simule un batch size plus grand (4x4 = 16)\n",
    "    warmup_ratio=0.1,                         # warmup sur 10% des steps pour stabiliser l'entraînement\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,              # récupère le meilleur modèle (selon eval loss)\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=2,                 # légère parallélisation de l’I/O\n",
    "    report_to=\"none\",                         # désactive Weights & Biases si non utilisé\n",
    ")\n",
    "\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    return np.all(y_true == y_pred, axis=1).mean()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    thresholds = np.linspace(0.1, 0.99, 20)\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (logits >= t).astype(int)\n",
    "        micro_precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
    "        if micro_precision > best_precision:\n",
    "            best_precision = micro_precision\n",
    "            best_threshold = t\n",
    "\n",
    "    # Final prediction with the best threshold\n",
    "    preds = (logits >= best_threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        'threshold': best_threshold,\n",
    "        'micro_precision': precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_recall': recall_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_f1': f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        'hamming_loss': hamming_loss(labels, preds),\n",
    "        'exact_match_accuracy': exact_match_accuracy(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train_val[\"train\"],\n",
    "    eval_dataset=dataset_train_val[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Préparez votre DataFrame de test :\n",
    "#    - Renommez la colonne target en labels\n",
    "#    - Transformez chaque liste de labels en array float32\n",
    "df_test = data_test.rename(columns={\"target\": \"labels\"})\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# 2. Créez un Dataset Hugging Face\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# 3. Définissez la même fonction de tokenisation que pour l’entraînement\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"case_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# 4. Appliquez la tokenisation\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 5. Facultatif : fixez le format PyTorch pour éviter d’avoir à convertir à la main\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 6a. Évaluation simple : renvoie loss + métriques de compute_metrics\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Résultats sur test :\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3350' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3350/3350 : < :, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3350, training_loss=0.0, metrics={'train_runtime': 0.4082, 'train_samples_per_second': 131291.268, 'train_steps_per_second': 8207.235, 'total_flos': 1.4136845312249856e+16, 'train_loss': 0.0, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"./results/checkpoint-3350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2680' max='2680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2680/2680 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_threshold</th>\n",
       "      <th>eval_micro_precision</th>\n",
       "      <th>eval_micro_recall</th>\n",
       "      <th>eval_micro_f1</th>\n",
       "      <th>eval_hamming_loss</th>\n",
       "      <th>eval_exact_match_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052049</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.994627</td>\n",
       "      <td>0.777871</td>\n",
       "      <td>0.872996</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.564658</td>\n",
       "      <td>115.6827</td>\n",
       "      <td>92.65</td>\n",
       "      <td>23.167</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_threshold  eval_micro_precision  eval_micro_recall  \\\n",
       "0   0.052049            0.99              0.994627           0.777871   \n",
       "\n",
       "   eval_micro_f1  eval_hamming_loss  eval_exact_match_accuracy  eval_runtime  \\\n",
       "0       0.872996           0.022227                   0.564658      115.6827   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  epoch  \n",
       "0                    92.65                 23.167    5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([metrics])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mon_modele_final/tokenizer_config.json',\n",
       " './mon_modele_final/special_tokens_map.json',\n",
       " './mon_modele_final/vocab.txt',\n",
       " './mon_modele_final/added_tokens.json',\n",
       " './mon_modele_final/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.save_model(\".././mon_modele_final2\")\n",
    "tokenizer.save_pretrained(\".././mon_modele_final2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A 68-year-old man presents with a persistent cough, present for three months, accompanied by increasing shortness of breath when he exerts himself. He also complains of recent lower back pain.\n",
    "He has a significant smoking history, having smoked the equivalent of 40 packs of cigarettes per year. Notably, he reports coughing up sputum tinged with blood on occasion. During the physical examination, \n",
    "the physician observes diminished breath sounds specifically in the lower portion of his right lung. An initial chest X-ray reveals a concerning mass located in the right lower lobe of the lung.\n",
    "To further investigate, a CT scan of the chest is performed. This imaging confirms the presence of a 4-centimeter mass within the right lower lobe. Additionally, \n",
    "the scan reveals enlarged lymph nodes in the region of the lung's hilum (hilar lymphadenopathy). Upon further questioning, the patient admits to experiencing nocturia, characterized by the need to urinate frequently during the night, \n",
    "approximately two to three times per night, over the past six months. He initially attributed this to simply drinking more fluids before bed. He also mentions mild, intermittent lower back pain that sometimes radiates down his right leg. \n",
    "He had previously dismissed this pain as a normal consequence of aging and stiffness. His medical history includes high blood pressure (hypertension), which is currently being managed with medication. An electrocardiogram (ECG) is performed as part of the evaluation. \n",
    "The ECG reveals a left bundle branch block, which is a new finding compared to previous ECG recordings. An echocardiogram shows mild left ventricular hypertrophy. To determine the specific nature of the lung mass and assess the involvement of the lymph nodes, \n",
    "the patient is scheduled for a bronchoscopy with a biopsy. In addition, due to his reported nocturia and lower back pain, \n",
    "a prostate-specific antigen (PSA) test will be performed to evaluate prostate health. A more comprehensive cardiac assessment is planned to further investigate the newly identified left bundle branch block.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA 68-year-old man presents with a persistent cough, present for three months, accompanied by increasing shortness of breath when he exerts himself. He also complains of recent lower back pain.\\nHe has a significant smoking history, having smoked the equivalent of 40 packs of cigarettes per year. Notably, he reports coughing up sputum tinged with blood on occasion. During the physical examination, \\nthe physician observes diminished breath sounds specifically in the lower portion of his right lung. An initial chest X-ray reveals a concerning mass located in the right lower lobe of the lung.\\nTo further investigate, a CT scan of the chest is performed. This imaging confirms the presence of a 4-centimeter mass within the right lower lobe. Additionally, \\nthe scan reveals enlarged lymph nodes in the region of the lung's hilum (hilar lymphadenopathy). Upon further questioning, the patient admits to experiencing nocturia, characterized by the need to urinate frequently during the night, \\napproximately two to three times per night, over the past six months. He initially attributed this to simply drinking more fluids before bed. He also mentions mild, intermittent lower back pain that sometimes radiates down his right leg. \\nHe had previously dismissed this pain as a normal consequence of aging and stiffness. His medical history includes high blood pressure (hypertension), which is currently being managed with medication. An electrocardiogram (ECG) is performed as part of the evaluation. \\nThe ECG reveals a left bundle branch block, which is a new finding compared to previous ECG recordings. An echocardiogram shows mild left ventricular hypertrophy. To determine the specific nature of the lung mass and assess the involvement of the lymph nodes, \\nthe patient is scheduled for a bronchoscopy with a biopsy. In addition, due to his reported nocturia and lower back pain, \\na prostate-specific antigen (PSA) test will be performed to evaluate prostate health. A more comprehensive cardiac assessment is planned to further investigate the newly identified left bundle branch block.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "# Mettre le modèle en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Prédiction (désactive le calcul de gradients)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Appliquer une sigmoïde pour obtenir les probabilités\n",
    "probs = torch.sigmoid(logits)\n",
    "\n",
    "# Seuil pour dire si chaque label est actif ou pas (ici 0.5)\n",
    "predicted_labels = (probs > 0.5).squeeze().bool().tolist()\n",
    "\n",
    "# Affichage\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C04 – neoplasms',\n",
       " 'C08 – respiratory tract diseases',\n",
       " 'C14 – cardiovascular diseases']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multilabel_preprocessing as mp\n",
    "mp.mesh_labels_from_vector(np.array(predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Étape                  | Outils                         | Ce que tu fais                            |\n",
    "|------------------------|--------------------------------|--------------------------------------------|\n",
    "| Choix du modèle        | HuggingFace `transformers`     | Utilise un BERT médical pré-entraîné       |\n",
    "| Préparation des données| `datasets`, `tokenizer`        | Tokenisation + conversion des labels       |\n",
    "| Modélisation           | `AutoModelForSequenceClassification` | Déclare une classification multi-label |\n",
    "| Entraînement           | `Trainer`                      | Fine-tuning du modèle sur tes données      |\n",
    "| Évaluation             | `f1_score`, `hamming_loss`     | Calcul des performances globales           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"⛔ Cette cellule ne doit pas être exécutée.\")\n",
    "\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "BUCKET_OUT = \"s3://quentin1999/Data_Projet_NLP\"\n",
    "FILE_KEY_OUT_S3 = \"df_target_V3.pkl\"\n",
    "FILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'wb') as file_out:\n",
    "    df.to_pickle(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"⛔ Cette cellule ne doit pas être exécutée.\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrer le modèle BERT trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle sauvegardé dans le Vault S3 : s3://quentin1999/Data_Projet_NLP/mon_modele_final.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# === 1. Zippage du dossier ===\n",
    "output_dir = \".././mon_modele_final\"\n",
    "zip_path = \".././mon_modele_final.zip\"\n",
    "shutil.make_archive(base_name=\"mon_modele_final\", format='zip', root_dir=output_dir)\n",
    "\n",
    "# === 2. Envoi vers S3 Vault ===\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "BUCKET_OUT = \"s3://quentin1999/Data_Projet_NLP\"\n",
    "MODEL_ZIP_KEY = \"mon_modele_final.zip\"\n",
    "MODEL_ZIP_PATH_S3 = BUCKET_OUT + \"/\" + MODEL_ZIP_KEY\n",
    "\n",
    "with fs.open(MODEL_ZIP_PATH_S3, 'wb') as f_out:\n",
    "    with open(zip_path, 'rb') as f_in:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"✅ Modèle sauvegardé dans le Vault S3 :\", MODEL_ZIP_PATH_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement depuis S3\n",
    "with fs.open(MODEL_ZIP_PATH_S3, 'rb') as f_in:\n",
    "    with open(\"mon_modele_final.zip\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Dézippage\n",
    "shutil.unpack_archive(\"mon_modele_final.zip\", extract_dir=\"./mon_modele_final\")\n",
    "\n",
    "# Chargement\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n",
      "\u001b[32m      7\u001b[39m model.eval()\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Exemple si tu as X_test sous forme de textes\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mX_test\u001b[49m):  \u001b[38;5;66;03m# ou DataLoader, selon ta structure\u001b[39;00m\n",
      "\u001b[32m     11\u001b[39m     inputs = tokenizer(batch, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m).to(model.device)\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Mettre le modèle en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Exemple si tu as X_test sous forme de textes\n",
    "for batch in tqdm(X_test):  # ou DataLoader, selon ta structure\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu().numpy()\n",
    "    y_pred_logits.append(logits)\n",
    "\n",
    "# 3. Empiler les logits et binariser\n",
    "y_pred_logits = np.vstack(y_pred_logits)         # (n_samples, n_classes)\n",
    "y_pred = (y_pred_logits >= 0.5).astype(int)      # Seuil de 0.5 pour binariser\n",
    "\n",
    "# 4. Évaluation (en supposant que y_test_array est déjà binairisé)\n",
    "f1_micro = f1_score(y_test_array, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test_array, y_pred, average='macro')\n",
    "hamming = hamming_loss(y_test_array, y_pred)\n",
    "\n",
    "print(f\"✅ F1 Micro : {f1_micro:.4f}\")\n",
    "print(f\"✅ F1 Macro : {f1_macro:.4f}\")\n",
    "print(f\"🔁 Hamming Loss : {hamming:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
