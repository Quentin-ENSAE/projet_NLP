{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Files removed: 6 (18.4 MB)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip cache purge\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Barre de progression pour Jupyter\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import s3fs\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/projet_NLP/Notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('../data/df_train.pkl')\n",
    "data_test = pd.read_pickle('../data/df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # doit renvoyer True\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))  # nom de ton GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Nom du mod√®le m√©dical BERT\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tu dois d√©finir le nombre de classes en sortie\n",
    "n_labels = 26  # par exemple\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=n_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     9646.00000\n",
      "mean       892.17458\n",
      "std        628.19550\n",
      "min         32.00000\n",
      "25%        491.00000\n",
      "50%        758.00000\n",
      "75%       1111.00000\n",
      "max      13554.00000\n",
      "Name: n_tokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculer le nombre de tokens pour chaque case_text\n",
    "stat=pd.DataFrame()\n",
    "stat['n_tokens'] = data_train['case_text'].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "\n",
    "# 2. Afficher les stats descriptives\n",
    "print(stat['n_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee1693374fb43ffa08744bc375d1880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "df = data_train.rename(columns={\"target\": \"labels\"})\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"case_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille dataset entrainement : 8574\n",
      "taille dataset validation : 1072\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "dataset_train_val = dataset.train_test_split(test_size=0.1/0.9) # 0.1 /0.9 pour avoir meme taille de validation set et de test set\n",
    "\n",
    "print(\"taille dataset entrainement :\", dataset_train_val[\"train\"].shape[0])\n",
    "print(\"taille dataset validation :\", dataset_train_val[\"test\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5360' max='5360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5360/5360 1:10:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.231641</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.834225</td>\n",
       "      <td>0.116985</td>\n",
       "      <td>0.205196</td>\n",
       "      <td>0.086718</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>13.840400</td>\n",
       "      <td>77.454000</td>\n",
       "      <td>19.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.176416</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.831487</td>\n",
       "      <td>0.394076</td>\n",
       "      <td>0.534724</td>\n",
       "      <td>0.065621</td>\n",
       "      <td>0.194963</td>\n",
       "      <td>13.800400</td>\n",
       "      <td>77.679000</td>\n",
       "      <td>19.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.161549</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.798799</td>\n",
       "      <td>0.498688</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.059989</td>\n",
       "      <td>0.230410</td>\n",
       "      <td>13.740000</td>\n",
       "      <td>78.020000</td>\n",
       "      <td>19.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.789089</td>\n",
       "      <td>0.526059</td>\n",
       "      <td>0.631271</td>\n",
       "      <td>0.058805</td>\n",
       "      <td>0.245336</td>\n",
       "      <td>13.735500</td>\n",
       "      <td>78.046000</td>\n",
       "      <td>19.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.155755</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.763328</td>\n",
       "      <td>0.574428</td>\n",
       "      <td>0.655541</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>13.802100</td>\n",
       "      <td>77.669000</td>\n",
       "      <td>19.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.766431</td>\n",
       "      <td>0.568429</td>\n",
       "      <td>0.652745</td>\n",
       "      <td>0.057872</td>\n",
       "      <td>0.254664</td>\n",
       "      <td>13.844500</td>\n",
       "      <td>77.431000</td>\n",
       "      <td>19.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.159459</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.755299</td>\n",
       "      <td>0.587927</td>\n",
       "      <td>0.661185</td>\n",
       "      <td>0.057656</td>\n",
       "      <td>0.269590</td>\n",
       "      <td>13.833000</td>\n",
       "      <td>77.496000</td>\n",
       "      <td>19.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.160754</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.747056</td>\n",
       "      <td>0.594676</td>\n",
       "      <td>0.662213</td>\n",
       "      <td>0.058051</td>\n",
       "      <td>0.258396</td>\n",
       "      <td>13.635600</td>\n",
       "      <td>78.618000</td>\n",
       "      <td>19.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.161919</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.740942</td>\n",
       "      <td>0.613423</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.057513</td>\n",
       "      <td>0.260261</td>\n",
       "      <td>13.642700</td>\n",
       "      <td>78.577000</td>\n",
       "      <td>19.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.163144</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.742054</td>\n",
       "      <td>0.604049</td>\n",
       "      <td>0.665978</td>\n",
       "      <td>0.057979</td>\n",
       "      <td>0.262127</td>\n",
       "      <td>13.738500</td>\n",
       "      <td>78.029000</td>\n",
       "      <td>19.507000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5360, training_loss=0.13675890336952992, metrics={'train_runtime': 4223.0675, 'train_samples_per_second': 20.303, 'train_steps_per_second': 1.269, 'total_flos': 2.256400307294208e+16, 'train_loss': 0.13675890336952992, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results_Bio_BERT\",\n",
    "    evaluation_strategy=\"epoch\",         # √©valuation √† chaque √©poque\n",
    "    save_strategy=\"epoch\",               # checkpoint √† chaque √©poque\n",
    "    save_total_limit=2,                  # on ne garde que les 2 derniers\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,                 # on peut tenter d‚Äôaugmenter\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",    # ‚Üê on surveille le micro-F1\n",
    "    greater_is_better=True,              # ‚Üê plus c‚Äôest grand mieux c‚Äôest\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type=\"linear\",          # on explicite le scheduler\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    thresholds = np.linspace(0.1, 0.99, 20)\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # on choisi le seuil qui maximise micro-F1\n",
    "    for t in thresholds:\n",
    "        preds = (logits >= t).astype(int)\n",
    "        f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "\n",
    "    # on calcule les m√©triques finales avec ce seuil optimal\n",
    "    preds = (logits >= best_threshold).astype(int)\n",
    "    return {\n",
    "        'threshold': best_threshold,\n",
    "        'micro_precision': precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_recall':    recall_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_f1':        f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        'hamming_loss':    hamming_loss(labels, preds),\n",
    "        'exact_match':     np.mean(np.all(labels == preds, axis=1))\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train_val['train'],\n",
    "    eval_dataset=dataset_train_val['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # stop si pas d‚Äôam√©lioration\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50892ebff03e4c979d6942d9c09525ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [268/268 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats sur test : {'eval_loss': 0.16995590925216675, 'eval_threshold': 0.1, 'eval_micro_precision': 0.7543702375616316, 'eval_micro_recall': 0.6, 'eval_micro_f1': 0.6683876092136616, 'eval_hamming_loss': 0.05991676234213548, 'eval_exact_match': 0.24720149253731344, 'eval_runtime': 13.6605, 'eval_samples_per_second': 78.474, 'eval_steps_per_second': 19.619, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# 1. Pr√©parez votre DataFrame de test :\n",
    "#    - Renommez la colonne target en labels\n",
    "#    - Transformez chaque liste de labels en array float32\n",
    "df_test = data_test.rename(columns={\"target\": \"labels\"})\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# 2. Cr√©ez un Dataset Hugging Face\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# 3. D√©finissez la m√™me fonction de tokenisation que pour l‚Äôentra√Ænement\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"case_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# 4. Appliquez la tokenisation\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 5. Facultatif : fixez le format PyTorch pour √©viter d‚Äôavoir √† convertir √† la main\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 6a. √âvaluation simple : renvoie loss + m√©triques de compute_metrics\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"R√©sultats sur test :\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3350' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3350/3350 : < :, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3350, training_loss=0.0, metrics={'train_runtime': 0.4082, 'train_samples_per_second': 131291.268, 'train_steps_per_second': 8207.235, 'total_flos': 1.4136845312249856e+16, 'train_loss': 0.0, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trainer.train(resume_from_checkpoint=\"./results_bio_BERT/checkpoint-3350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2680' max='2680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2680/2680 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_threshold</th>\n",
       "      <th>eval_micro_precision</th>\n",
       "      <th>eval_micro_recall</th>\n",
       "      <th>eval_micro_f1</th>\n",
       "      <th>eval_hamming_loss</th>\n",
       "      <th>eval_exact_match_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052049</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.994627</td>\n",
       "      <td>0.777871</td>\n",
       "      <td>0.872996</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.564658</td>\n",
       "      <td>115.6827</td>\n",
       "      <td>92.65</td>\n",
       "      <td>23.167</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_threshold  eval_micro_precision  eval_micro_recall  \\\n",
       "0   0.052049            0.99              0.994627           0.777871   \n",
       "\n",
       "   eval_micro_f1  eval_hamming_loss  eval_exact_match_accuracy  eval_runtime  \\\n",
       "0       0.872996           0.022227                   0.564658      115.6827   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  epoch  \n",
       "0                    92.65                 23.167    5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([metrics])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mon_modele_final/tokenizer_config.json',\n",
       " './mon_modele_final/special_tokens_map.json',\n",
       " './mon_modele_final/vocab.txt',\n",
       " './mon_modele_final/added_tokens.json',\n",
       " './mon_modele_final/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.save_model(\".././mon_modele_final2\")\n",
    "tokenizer.save_pretrained(\".././mon_modele_final2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A 68-year-old man presents with a persistent cough, present for three months, accompanied by increasing shortness of breath when he exerts himself. He also complains of recent lower back pain.\n",
    "He has a significant smoking history, having smoked the equivalent of 40 packs of cigarettes per year. Notably, he reports coughing up sputum tinged with blood on occasion. During the physical examination, \n",
    "the physician observes diminished breath sounds specifically in the lower portion of his right lung. An initial chest X-ray reveals a concerning mass located in the right lower lobe of the lung.\n",
    "To further investigate, a CT scan of the chest is performed. This imaging confirms the presence of a 4-centimeter mass within the right lower lobe. Additionally, \n",
    "the scan reveals enlarged lymph nodes in the region of the lung's hilum (hilar lymphadenopathy). Upon further questioning, the patient admits to experiencing nocturia, characterized by the need to urinate frequently during the night, \n",
    "approximately two to three times per night, over the past six months. He initially attributed this to simply drinking more fluids before bed. He also mentions mild, intermittent lower back pain that sometimes radiates down his right leg. \n",
    "He had previously dismissed this pain as a normal consequence of aging and stiffness. His medical history includes high blood pressure (hypertension), which is currently being managed with medication. An electrocardiogram (ECG) is performed as part of the evaluation. \n",
    "The ECG reveals a left bundle branch block, which is a new finding compared to previous ECG recordings. An echocardiogram shows mild left ventricular hypertrophy. To determine the specific nature of the lung mass and assess the involvement of the lymph nodes, \n",
    "the patient is scheduled for a bronchoscopy with a biopsy. In addition, due to his reported nocturia and lower back pain, \n",
    "a prostate-specific antigen (PSA) test will be performed to evaluate prostate health. A more comprehensive cardiac assessment is planned to further investigate the newly identified left bundle branch block.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA 68-year-old man presents with a persistent cough, present for three months, accompanied by increasing shortness of breath when he exerts himself. He also complains of recent lower back pain.\\nHe has a significant smoking history, having smoked the equivalent of 40 packs of cigarettes per year. Notably, he reports coughing up sputum tinged with blood on occasion. During the physical examination, \\nthe physician observes diminished breath sounds specifically in the lower portion of his right lung. An initial chest X-ray reveals a concerning mass located in the right lower lobe of the lung.\\nTo further investigate, a CT scan of the chest is performed. This imaging confirms the presence of a 4-centimeter mass within the right lower lobe. Additionally, \\nthe scan reveals enlarged lymph nodes in the region of the lung's hilum (hilar lymphadenopathy). Upon further questioning, the patient admits to experiencing nocturia, characterized by the need to urinate frequently during the night, \\napproximately two to three times per night, over the past six months. He initially attributed this to simply drinking more fluids before bed. He also mentions mild, intermittent lower back pain that sometimes radiates down his right leg. \\nHe had previously dismissed this pain as a normal consequence of aging and stiffness. His medical history includes high blood pressure (hypertension), which is currently being managed with medication. An electrocardiogram (ECG) is performed as part of the evaluation. \\nThe ECG reveals a left bundle branch block, which is a new finding compared to previous ECG recordings. An echocardiogram shows mild left ventricular hypertrophy. To determine the specific nature of the lung mass and assess the involvement of the lymph nodes, \\nthe patient is scheduled for a bronchoscopy with a biopsy. In addition, due to his reported nocturia and lower back pain, \\na prostate-specific antigen (PSA) test will be performed to evaluate prostate health. A more comprehensive cardiac assessment is planned to further investigate the newly identified left bundle branch block.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "# Mettre le mod√®le en mode √©valuation\n",
    "model.eval()\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Pr√©diction (d√©sactive le calcul de gradients)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Appliquer une sigmo√Øde pour obtenir les probabilit√©s\n",
    "probs = torch.sigmoid(logits)\n",
    "\n",
    "# Seuil pour dire si chaque label est actif ou pas (ici 0.5)\n",
    "predicted_labels = (probs > 0.5).squeeze().bool().tolist()\n",
    "\n",
    "# Affichage\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C04 ‚Äì neoplasms',\n",
       " 'C08 ‚Äì respiratory tract diseases',\n",
       " 'C14 ‚Äì cardiovascular diseases']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multilabel_preprocessing as mp\n",
    "mp.mesh_labels_from_vector(np.array(predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| √âtape                  | Outils                         | Ce que tu fais                            |\n",
    "|------------------------|--------------------------------|--------------------------------------------|\n",
    "| Choix du mod√®le        | HuggingFace `transformers`     | Utilise un BERT m√©dical pr√©-entra√Æn√©       |\n",
    "| Pr√©paration des donn√©es| `datasets`, `tokenizer`        | Tokenisation + conversion des labels       |\n",
    "| Mod√©lisation           | `AutoModelForSequenceClassification` | D√©clare une classification multi-label |\n",
    "| Entra√Ænement           | `Trainer`                      | Fine-tuning du mod√®le sur tes donn√©es      |\n",
    "| √âvaluation             | `f1_score`, `hamming_loss`     | Calcul des performances globales           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"‚õî Cette cellule ne doit pas √™tre ex√©cut√©e.\")\n",
    "\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "BUCKET_OUT = \"s3://quentin1999/Data_Projet_NLP\"\n",
    "FILE_KEY_OUT_S3 = \"df_target_V3.pkl\"\n",
    "FILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, 'wb') as file_out:\n",
    "    df.to_pickle(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"‚õî Cette cellule ne doit pas √™tre ex√©cut√©e.\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrer le mod√®le BERT trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le sauvegard√© dans le Vault S3 : s3://quentin1999/Data_Projet_NLP/mon_modele_final.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# === 1. Zippage du dossier ===\n",
    "output_dir = \".././mon_modele_final\"\n",
    "zip_path = \".././mon_modele_final.zip\"\n",
    "shutil.make_archive(base_name=\"mon_modele_final\", format='zip', root_dir=output_dir)\n",
    "\n",
    "# === 2. Envoi vers S3 Vault ===\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "BUCKET_OUT = \"s3://quentin1999/Data_Projet_NLP\"\n",
    "MODEL_ZIP_KEY = \"mon_modele_final.zip\"\n",
    "MODEL_ZIP_PATH_S3 = BUCKET_OUT + \"/\" + MODEL_ZIP_KEY\n",
    "\n",
    "with fs.open(MODEL_ZIP_PATH_S3, 'wb') as f_out:\n",
    "    with open(zip_path, 'rb') as f_in:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© dans le Vault S3 :\", MODEL_ZIP_PATH_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©chargement depuis S3\n",
    "with fs.open(MODEL_ZIP_PATH_S3, 'rb') as f_in:\n",
    "    with open(\"mon_modele_final.zip\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# D√©zippage\n",
    "shutil.unpack_archive(\"mon_modele_final.zip\", extract_dir=\"./mon_modele_final\")\n",
    "\n",
    "# Chargement\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./mon_modele_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mon_modele_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n",
      "\u001b[32m      7\u001b[39m model.eval()\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Exemple si tu as X_test sous forme de textes\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mX_test\u001b[49m):  \u001b[38;5;66;03m# ou DataLoader, selon ta structure\u001b[39;00m\n",
      "\u001b[32m     11\u001b[39m     inputs = tokenizer(batch, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m).to(model.device)\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Mettre le mod√®le en mode √©valuation\n",
    "model.eval()\n",
    "\n",
    "# Exemple si tu as X_test sous forme de textes\n",
    "for batch in tqdm(X_test):  # ou DataLoader, selon ta structure\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu().numpy()\n",
    "    y_pred_logits.append(logits)\n",
    "\n",
    "# 3. Empiler les logits et binariser\n",
    "y_pred_logits = np.vstack(y_pred_logits)         # (n_samples, n_classes)\n",
    "y_pred = (y_pred_logits >= 0.5).astype(int)      # Seuil de 0.5 pour binariser\n",
    "\n",
    "# 4. √âvaluation (en supposant que y_test_array est d√©j√† binairis√©)\n",
    "f1_micro = f1_score(y_test_array, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test_array, y_pred, average='macro')\n",
    "hamming = hamming_loss(y_test_array, y_pred)\n",
    "\n",
    "print(f\"‚úÖ F1 Micro : {f1_micro:.4f}\")\n",
    "print(f\"‚úÖ F1 Macro : {f1_macro:.4f}\")\n",
    "print(f\"üîÅ Hamming Loss : {hamming:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
